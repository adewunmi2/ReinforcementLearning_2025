{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gymnaasium\n",
    "# pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "# pip install ale-py\n",
    "# pip install stable-baselines3\n",
    "# pip install stable-baselines3[extra]\n",
    "# pip install tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt for Riverraid:\n",
    "\n",
    "# I want to train an RL agent with stable baselines. Riverraid as the environment \n",
    "# (Farama Gymnasium). I'm using CPU for training, so the goal of the agent is to survive \n",
    "# 10 seconds. The total training time should be less than 45 minutes on a CPU.\n",
    "\n",
    "# 2nd prompt:\n",
    "# Can I have a version where the agent has a custom reward for staying alive for 10 seconds at least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium \n",
    "import gymnasium as gym\n",
    "from gymnasium.utils import play\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# import stable baselines => PPO as the base algorithm\n",
    "# Dummy vector environment => handles image data\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import time\n",
    "#import matplotlib.pyplot as plt\n",
    "#import time\n",
    "#from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Wrapper for the Riverraid in order to adjust the rewarding\n",
    "# towards the goal => survive for 10 seconds in the game\n",
    "class CustomAssaultV5(gym.Env):\n",
    "# class CustomRiverraidV5(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CustomAssaultV5, self).__init__()\n",
    "        # super(CustomRiverraidV5, self).__init__()\n",
    "        self.env = gym.make('ALE/Assault-v5') \n",
    "        # self.env = gym.make('ALE/Riverraid-v5')  # Change to Riverraid-v5\n",
    "        self.time_alive = 0  # Track survival time\n",
    "\n",
    "        # Make sure the action and observation spaces are the same as the original environment\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "        \n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.time_alive = 0  # Reset survival time at the start of each episode\n",
    "        return self.env.reset(*args, **kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Perform one step in the original Riverraid environment\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Increase survival time on each step if the agent is still alive\n",
    "        if not done:\n",
    "            self.time_alive += 1\n",
    "        else:\n",
    "            self.time_alive = 0  # Reset the timer if the agent dies\n",
    "\n",
    "        # Custom reward: reward for surviving 10 seconds\n",
    "        if self.time_alive >= 10:\n",
    "            reward += 1  # Give a bonus reward for surviving 10 seconds\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render(*args, **kwargs)\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "\n",
    "# Create the custom Riverraid-v5 environment\n",
    "env = CustomAssaultV5()\n",
    "# env = CustomRiverraidV5()\n",
    "\n",
    "# Wrap it for vectorized environments (important for Stable Baselines3)\n",
    "env = DummyVecEnv([lambda: env])  # Vectorized environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 65   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 31   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 296        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41712973 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.74      |\n",
      "|    explained_variance   | -0.00035   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 390        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.0854     |\n",
      "|    value_loss           | 1.91e+03   |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 12        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 510       |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8311552 |\n",
      "|    clip_fraction        | 0.749     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.34     |\n",
      "|    explained_variance   | 0.342     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 292       |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | 0.164     |\n",
      "|    value_loss           | 1.08e+03  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 11        |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 740       |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.1941314 |\n",
      "|    clip_fraction        | 0.873     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.56     |\n",
      "|    explained_variance   | 0.137     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 217       |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | 0.304     |\n",
      "|    value_loss           | 648       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 10       |\n",
      "|    iterations           | 5        |\n",
      "|    time_elapsed         | 959      |\n",
      "|    total_timesteps      | 10240    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.677837 |\n",
      "|    clip_fraction        | 0.798    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.9     |\n",
      "|    explained_variance   | 0.418    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 199      |\n",
      "|    n_updates            | 40       |\n",
      "|    policy_gradient_loss | 0.125    |\n",
      "|    value_loss           | 526      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 10        |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 1183      |\n",
      "|    total_timesteps      | 12288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0684434 |\n",
      "|    clip_fraction        | 0.371     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.51     |\n",
      "|    explained_variance   | 0.766     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 72.6      |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | 0.0076    |\n",
      "|    value_loss           | 389       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 10         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 1407       |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14608704 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.672     |\n",
      "|    explained_variance   | 0.841      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 199        |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | 0.0258     |\n",
      "|    value_loss           | 379        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 1656       |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16388157 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.423     |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 163        |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | 0.025      |\n",
      "|    value_loss           | 261        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 9         |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 1881      |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7967269 |\n",
      "|    clip_fraction        | 0.41      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.558    |\n",
      "|    explained_variance   | 0.587     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 140       |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | 0.0622    |\n",
      "|    value_loss           | 545       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 2103       |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32198304 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.714     |\n",
      "|    explained_variance   | 0.811      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 96.1       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | 0.0678     |\n",
      "|    value_loss           | 332        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 9         |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 2323      |\n",
      "|    total_timesteps      | 22528     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.8359733 |\n",
      "|    clip_fraction        | 0.693     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.418    |\n",
      "|    explained_variance   | 0.59      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 167       |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | 0.125     |\n",
      "|    value_loss           | 369       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 2553       |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08723094 |\n",
      "|    clip_fraction        | 0.0307     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0332    |\n",
      "|    explained_variance   | 0.649      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.67       |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | 0.0156     |\n",
      "|    value_loss           | 81.7       |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 9             |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 2775          |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5390258e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00401      |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.89          |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | 1.16e-05      |\n",
      "|    value_loss           | 20.2          |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 9         |\n",
      "|    iterations           | 14        |\n",
      "|    time_elapsed         | 3004      |\n",
      "|    total_timesteps      | 28672     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2235203 |\n",
      "|    clip_fraction        | 0.0557    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0845   |\n",
      "|    explained_variance   | 0.933     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 20.8      |\n",
      "|    n_updates            | 130       |\n",
      "|    policy_gradient_loss | -0.00435  |\n",
      "|    value_loss           | 25.3      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 9         |\n",
      "|    iterations           | 15        |\n",
      "|    time_elapsed         | 3239      |\n",
      "|    total_timesteps      | 30720     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6843584 |\n",
      "|    clip_fraction        | 0.185     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0619   |\n",
      "|    explained_variance   | 0.246     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 351       |\n",
      "|    n_updates            | 140       |\n",
      "|    policy_gradient_loss | 0.0572    |\n",
      "|    value_loss           | 894       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 9         |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 3472      |\n",
      "|    total_timesteps      | 32768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.92e-09 |\n",
      "|    explained_variance   | 0.296     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.33      |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | 1.2e-07   |\n",
      "|    value_loss           | 29.8      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 9         |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 3695      |\n",
      "|    total_timesteps      | 34816     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.48e-08 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.636     |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | 0.00407   |\n",
      "|    value_loss           | 15.1      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 9         |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 3932      |\n",
      "|    total_timesteps      | 36864     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.27e-07 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.57      |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | 0.000618  |\n",
      "|    value_loss           | 10.8      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 9         |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 4169      |\n",
      "|    total_timesteps      | 38912     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.11e-05 |\n",
      "|    explained_variance   | 0.902     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.3       |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | 1.34e-07  |\n",
      "|    value_loss           | 21.5      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 4421       |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00417479 |\n",
      "|    clip_fraction        | 0.000488   |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00139   |\n",
      "|    explained_variance   | 0.647      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1          |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | 0.00299    |\n",
      "|    value_loss           | 22.2       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 8         |\n",
      "|    iterations           | 21        |\n",
      "|    time_elapsed         | 4801      |\n",
      "|    total_timesteps      | 43008     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.83e-05 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.816     |\n",
      "|    n_updates            | 200       |\n",
      "|    policy_gradient_loss | -0.00529  |\n",
      "|    value_loss           | 2.22      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# with my CPU, ~ 110k timesteps = ~ 40min (MLPPolicy, agent performance was HORRIBLE (crashes into a wall in a second))\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# with my GPU, ~ 120k timesteps = ~ 28-29min (CNNPolicy, which is heavier to train, due to CNN)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:207\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m approx_kl_divs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rollout_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[0;32m    208\u001b[0m     actions \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mactions\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:505\u001b[0m, in \u001b[0;36mRolloutBuffer.get\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    503\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs:\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m     start_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:521\u001b[0m, in \u001b[0;36mRolloutBuffer._get_samples\u001b[1;34m(self, batch_inds, env)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_samples\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     batch_inds: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m    511\u001b[0m     env: Optional[VecNormalize] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RolloutBufferSamples:\n\u001b[0;32m    513\u001b[0m     data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations[batch_inds],\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[batch_inds],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RolloutBufferSamples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:139\u001b[0m, in \u001b[0;36mBaseBuffer.to_torch\u001b[1;34m(self, array, copy)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03mConvert a numpy array to a PyTorch tensor.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03mNote: it copies the data by default\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39mas_tensor(array, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize PPO model, CnnPolicy is most likely better in \n",
    "# Atari environments than the basic MlpPolicy\n",
    "# model = PPO('CnnPolicy', env, verbose=1)\n",
    "model = PPO('MLPPolicy', env, verbose=1)\n",
    "\n",
    "# with my CPU, ~ 110k timesteps = ~ 40min (MLPPolicy, agent performance was HORRIBLE (crashes into a wall in a second))\n",
    "# with my GPU, ~ 120k timesteps = ~ 28-29min (CNNPolicy, which is heavier to train, due to CNN)\n",
    "model.learn(total_timesteps=110000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "# model.save(\"custom_assualt_v5_ppo\")\n",
    "model.save(\"custom_riverraid_v5_ppo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
