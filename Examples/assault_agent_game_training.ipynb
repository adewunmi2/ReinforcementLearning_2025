{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gymnaasium\n",
    "# pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "# pip install ale-py\n",
    "# pip install stable-baselines3\n",
    "# pip install stable-baselines3[extra]\n",
    "# pip install tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt for Riverraid:\n",
    "\n",
    "# I want to train an RL agent with stable baselines. Riverraid as the environment \n",
    "# (Farama Gymnasium). I'm using CPU for training, so the goal of the agent is to survive \n",
    "# 10 seconds. The total training time should be less than 45 minutes on a CPU.\n",
    "\n",
    "# 2nd prompt:\n",
    "# Can I have a version where the agent has a custom reward for staying alive for 10 seconds at least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium \n",
    "import gymnasium as gym\n",
    "from gymnasium.utils import play\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# import stable baselines => PPO as the base algorithm\n",
    "# Dummy vector environment => handles image data\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import time\n",
    "#import matplotlib.pyplot as plt\n",
    "#import time\n",
    "#from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Wrapper for the Riverraid in order to adjust the rewarding\n",
    "# towards the goal => survive for 10 seconds in the game\n",
    "# class CustomAssaultV5(gym.Env):\n",
    "class CustomRiverraidV5(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CustomRiverraidV5, self).__init__()\n",
    "        # self.env = gym.make('ALE/Assault-v5') \n",
    "        self.env = gym.make('ALE/Riverraid-v5')  # Change to Riverraid-v5\n",
    "        self.time_alive = 0  # Track survival time\n",
    "\n",
    "        # Make sure the action and observation spaces are the same as the original environment\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "        \n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.time_alive = 0  # Reset survival time at the start of each episode\n",
    "        return self.env.reset(*args, **kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Perform one step in the original Riverraid environment\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Increase survival time on each step if the agent is still alive\n",
    "        if not done:\n",
    "            self.time_alive += 1\n",
    "        else:\n",
    "            self.time_alive = 0  # Reset the timer if the agent dies\n",
    "\n",
    "        # Custom reward: reward for surviving 10 seconds\n",
    "        if self.time_alive >= 10:\n",
    "            reward += 1  # Give a bonus reward for surviving 10 seconds\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render(*args, **kwargs)\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "\n",
    "# Create the custom Riverraid-v5 environment\n",
    "# env = CustomAssaultV5()\n",
    "env = CustomRiverraidV5()\n",
    "\n",
    "# Wrap it for vectorized environments (important for Stable Baselines3)\n",
    "env = DummyVecEnv([lambda: env])  # Vectorized environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PPO model, CnnPolicy is most likely better in \n",
    "# Atari environments than the basic MlpPolicy\n",
    "model = PPO('CnnPolicy', env, verbose=1)\n",
    "\n",
    "# with my CPU, ~ 110k timesteps = ~ 40min (MLPPolicy, agent performance was HORRIBLE (crashes into a wall in a second))\n",
    "# with my GPU, ~ 120k timesteps = ~ 28-29min (CNNPolicy, which is heavier to train, due to CNN)\n",
    "model.learn(total_timesteps=120000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "# model.save(\"custom_assualt_v5_ppo\")\n",
    "model.save(\"custom_riverraid_v5_ppo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
