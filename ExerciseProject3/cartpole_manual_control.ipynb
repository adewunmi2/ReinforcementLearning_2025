{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The Cart Pole, DQN- Deep Q-learning Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more code here later\n",
    "\n",
    "# we are going to need keras and TORCH in this example\n",
    "# pip install keras\n",
    "# pip install torch==2.5.1\n",
    "# gymnasium.farama.org/environments/classic_control/cart_pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils import play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual playing the cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Cart Pole environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array', max_episode_steps=700)\n",
    "\n",
    "# map the keys, 0 => move cart towards left, 1 => move cart towards right\n",
    "action_keys = {\n",
    "    \"a\": 0,\n",
    "    \"d\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# I tried the changed the fps\n",
    "# from 30 to 10, to see how the game\n",
    "# behave playing manually\n",
    "# To see how the environment working.\n",
    "play.play(env, fps=10, keys_to_action=action_keys, wait_on_player=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Observation though, I think it is somehow could be balanced when the fps=10. But when fps > 10 it (Cartpole) moves faster so making it quite hard to balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-learning vs Classical Q-learning: in Deep Q-Learning it is associated with unique state such as 3D infinite while Classical Q-Learning is 2D. It is advisable to use Deep Q-Learning when the environment state space is complex and large i.e iniinite while in Classical Q-Learning the state space environment is simple i.e 2D. Also, Classical Q-Learning the Agent is trained to figure out every possible action in every state in the environment towards getting the reward in the Q-table while DQN Learning it often rely on learning by experience say understing the trend/pattern that occurred in the training process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer: also known as experience replay which helps the trained Agent to store the old experiences after every each action (either positive or negative) taken and then store the actions in a replay buffer \n",
    "#### Source: https://medium.com/@heyamit10/deep-reinforcement-learning-with-experience-replay-1222ea711897"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-network vs Target network: The Q-network is the main neural network that learn actively as it's training step by step and always updating during the training process of the model Agent. It takes the current state as input and outputs Q-values for every possible actions and, updating frequently. While Target network updating less frequently as it saves every best result from the Q-network. It generates target Q-values during training. and provides stabilize learning by preventing the target values from shifting too rapidly.\n",
    "\n",
    "#### Source: https://moodle.eoppimispalvelut.fi/pluginfile.php/2980225/mod_resource/content/2/RL_part2.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common neural networks (ANNs) vs DQN-networks: both the Artificial Neural Networks (ANN) and DQN-network are distinct in their depth and complexity such that ANNs are relatively shallow and are used for simpler tasks, while DNNs are characterized by their depth and are employed for complex pattern recognition tasks where hierarchical features are essential. ANNs are generally models used for tasks such as classification and regression. While Deep Q-Networks (DQNs), on the other hand, are used in reinforcement learning to approximate Q-values for state-action pairs. Inaddition, ANNs learn from labeled data, DQNs learn through interaction with the environment, using Q-learning and experience replay to update their predictions and improve decision-making.\n",
    "\n",
    "Sources: Google and chatGPT\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### DQN hyperparameters: this is a way of fine tuning a limited computation RL optimization by getting optimal performance from the trained agent model. I would say the main hyper-parameters are epsilon schedule, size of experience replay buffer, batch-size, number of iterations for which the target network is kept fixed. \n",
    "\n",
    "#### Source: https://www.oreilly.com/library/view/keras-deep-learning/9781788621755/8b709539-5643-4b98-97c7-2f934101c3cf.xhtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN training vs classical Q-learning training: I think in the two cases I would say it all depends on the environment. In Classical Q-Learning the Agent model trained and converge faster in a simple environment such as FrozenLake-v0. As Classical Q-learning's algorithm is simple and efficient, therefore, it runs a lot faster than training a neural network.\n",
    "### However, in a complex environment as atari:https://gym.openai.com/envs/Assault-v0/ as there are numerous states in the environment which takes quite longer time to train as it require a very large memory plus a very long training time to be able to enumerate and explore all possible states.\n",
    "\n",
    "#### Sources: https://stackoverflow.com/questions/67261599/convergence-time-of-q-learning-vs-deep-q-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
