{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from IPython.utils import io\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.utils import importstring\n",
    "\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The environment is created\n",
    "# https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "\n",
    "# remember: Cart Pole is extremely sensitive to inputs\n",
    "# you probably need a very low epsilon in the end of the training\n",
    "epsilon_min = 0.015\n",
    "\n",
    "# in DQN-networks we usually use a MULTIPLIER -version of decay\n",
    "# after each episode => multiply epsilon with 0.99 etc. instead \n",
    "# reducing the epsilon with a constant step-by-step reduction like 0.00025\n",
    "epsilon_decay = 0.9925\n",
    "\n",
    "# neural network-specific hyperparameters\n",
    "# please note: this is the neural network learning rate\n",
    "# we are going to skip the Q-learning learning rate because of this\n",
    "learning_rate = 0.01\n",
    "\n",
    "# this is the target \"goal\" of our agent\n",
    "# if the agent is able to play the environment for 350 steps WITHOUT failing => win the game\n",
    "max_steps = 500\n",
    "\n",
    "# similar to usual neural network batch size\n",
    "# the bigger the batch size, the faster the training and vice versa\n",
    "# by default, batch size should not affect the quality of the training\n",
    "# but in case of reinforcement learning, we'll have to double-check\n",
    "batch_size = 32 \n",
    "\n",
    "# \"max epochs\" -> episodes are basically the epochs of reinforcement learning\n",
    "max_episodes = 500\n",
    "\n",
    "# replay buffer capacity: usually simple environments work with small capacities \n",
    "# (1000-5000), more complex environments are usually 10000+ in capacity size\n",
    "buffer_capacity = 1000\n",
    "\n",
    "# Q-network and target network parameters\n",
    "# train_steps => how often the Q-network values are updated\n",
    "q_network_train_step_interval = 4\n",
    "\n",
    "# how often the target network values are updated\n",
    "target_step_update_interval = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Basic DQN replay buffer is usually more or less like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer / experience replay -class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "\n",
    "        # list that stores the experiences\n",
    "        self.buffer = []\n",
    "\n",
    "         # pointer to the current position in the buffer\n",
    "        self.position = 0\n",
    "\n",
    "    # if the buffer is full, the new experience will overwrite the oldest experience\n",
    "    def store(self, experience): \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = experience\n",
    "\n",
    "        # update the position pointer\n",
    "        self.position = (self.position + 1) % self.capacity \n",
    "        \n",
    "    # randomly sample a batch of experiences for our DQN\n",
    "    def sample(self, batch_size): \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the neural networks (Q-network + target network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a helper function to build a q-network\n",
    "def build_q_network():\n",
    "    # basic idea => observations go in, actions come out\n",
    "    # you can adjust the details of the network as you wish\n",
    "    # have more layers, more nodes etc. most of the same things\n",
    "    # we used already in Deep Learning\n",
    "    model = Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(state_dim,)),\n",
    "        layers.Dense(action_dim, activation='linear')\n",
    "    ])\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# initialize Q-network AND the target network\n",
    "q_network = build_q_network()\n",
    "\n",
    "# we use the same \"blueprint\" for the target network as the q-network\n",
    "# because is essentially a reference network to q-network\n",
    "target_network = build_q_network()\n",
    "\n",
    "# SYNCHRONIZE the initial state between both networks -> IMPORTANT STEP\n",
    "# otherwise the networks have different starting points\n",
    "# and the target network is not very helpful as a reference for the q-network\n",
    "target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "# INITIALIZE the replay buffer\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Helper functions: epsilon function + train step -function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for handling the epsilon (exploration vs. exploitation)\n",
    "# this basic epsilon implementation is known as \"epsilon-greedy\"\n",
    "# compare this to the typical exploration/exploitation\n",
    "# in previous FrozenLake and MountainCar, same principle\n",
    "# different implementation\n",
    "def select_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        # take a random action -> exploration\n",
    "        return np.random.choice(action_dim)\n",
    "    \n",
    "    # use the existing Q-values of the agent -> exploitation\n",
    "    # we have to match the state's data format into neural network -format\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "\n",
    "    # there's going to be lots of Keras/Torch -based extra message (output)\n",
    "    # use io.capture_output() to suppress them\n",
    "    with io.capture_output() as captured:\n",
    "        # this is basically the same as getting the best value from the current\n",
    "        # Q-table in classical Q-learning\n",
    "        q_values = q_network.predict(state)\n",
    "\n",
    "    # return best action decided by current model => either go left or right\n",
    "    return np.argmax(q_values)\n",
    "\n",
    "\n",
    "# training loop helper function for the DQN\n",
    "# in your exercise project 3 => this is a very good function to study \n",
    "# in more detail, since this is basically the way how we can adapt\n",
    "# the goals of reinforcement learning to work with basic neural networks as well\n",
    "def train_step():\n",
    "    # in the beginning of the training, we don't have enough data in\n",
    "    # the buffer (compared to batch size), we'll have to wait\n",
    "    if replay_buffer.size() < batch_size:\n",
    "        return\n",
    "    \n",
    "    # if we have enough samples, we continue here\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # COMPARE THIS ON how we unpack the observation\n",
    "    # in classical Q-learning, we just have a batch of observations this time\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    # predict Q-values for the current and next state using the Q-network\n",
    "    state_q_values = q_network.predict(states, verbose=0)\n",
    "    next_state_q_values = target_network.predict(next_states, verbose=0)\n",
    "\n",
    "    # take a copy of the state values so we can update them in the loop below\n",
    "    targets = state_q_values.copy()\n",
    "\n",
    "    # compute target q-values => basically same as Q-targets in Bellman/Q-learning -equation\n",
    "    # BUT WITHOUT LEARNING RATE\n",
    "    for i in range(batch_size):\n",
    "        if dones[i]:\n",
    "            targets[i, actions[i]] = rewards[i]\n",
    "        else:\n",
    "            # this is essentially the Bellman/Q-learning -equation, COMPARE TO EARLIER EXERCISES\n",
    "            targets[i, actions[i]] = rewards[i] + gamma * np.max(next_state_q_values[i])\n",
    "\n",
    "    # finally, update the Q-network based on the targets we just created \n",
    "    # \"out of nowhere\" -> from the past experiences in replay buffer\n",
    "    with io.capture_output() as captured:\n",
    "        loss = q_network.train_on_batch(states, targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop (uses helper functions + replay buffer from earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 32\n",
      "Episode 1: Total Reward = 32.00, Epsilon = 0.993\n",
      "Total steps: 46\n",
      "Episode 2: Total Reward = 14.00, Epsilon = 0.985\n",
      "Total steps: 69\n",
      "Episode 3: Total Reward = 23.00, Epsilon = 0.978\n",
      "Total steps: 81\n",
      "Episode 4: Total Reward = 12.00, Epsilon = 0.970\n",
      "Total steps: 95\n",
      "Episode 5: Total Reward = 14.00, Epsilon = 0.963\n",
      "Total steps: 112\n",
      "Episode 6: Total Reward = 17.00, Epsilon = 0.956\n",
      "Total steps: 133\n",
      "Episode 7: Total Reward = 21.00, Epsilon = 0.949\n",
      "Total steps: 151\n",
      "Episode 8: Total Reward = 18.00, Epsilon = 0.942\n",
      "Total steps: 177\n",
      "Episode 9: Total Reward = 26.00, Epsilon = 0.934\n",
      "Total steps: 191\n",
      "Episode 10: Total Reward = 14.00, Epsilon = 0.927\n",
      "Total steps: 210\n",
      "Episode 11: Total Reward = 19.00, Epsilon = 0.921\n",
      "Total steps: 233\n",
      "Episode 12: Total Reward = 23.00, Epsilon = 0.914\n",
      "Total steps: 245\n",
      "Episode 13: Total Reward = 12.00, Epsilon = 0.907\n",
      "Total steps: 258\n",
      "Episode 14: Total Reward = 13.00, Epsilon = 0.900\n",
      "Total steps: 273\n",
      "Episode 15: Total Reward = 15.00, Epsilon = 0.893\n",
      "Total steps: 288\n",
      "Episode 16: Total Reward = 15.00, Epsilon = 0.887\n",
      "Total steps: 315\n",
      "Episode 17: Total Reward = 27.00, Epsilon = 0.880\n",
      "Total steps: 337\n",
      "Episode 18: Total Reward = 22.00, Epsilon = 0.873\n",
      "Total steps: 353\n",
      "Episode 19: Total Reward = 16.00, Epsilon = 0.867\n",
      "Total steps: 379\n",
      "Episode 20: Total Reward = 26.00, Epsilon = 0.860\n",
      "Total steps: 395\n",
      "Episode 21: Total Reward = 16.00, Epsilon = 0.854\n",
      "Total steps: 405\n",
      "Episode 22: Total Reward = 10.00, Epsilon = 0.847\n",
      "Total steps: 420\n",
      "Episode 23: Total Reward = 15.00, Epsilon = 0.841\n",
      "Total steps: 437\n",
      "Episode 24: Total Reward = 17.00, Epsilon = 0.835\n",
      "Total steps: 468\n",
      "Episode 25: Total Reward = 31.00, Epsilon = 0.828\n",
      "Total steps: 480\n",
      "Episode 26: Total Reward = 12.00, Epsilon = 0.822\n",
      "Total steps: 493\n",
      "Episode 27: Total Reward = 13.00, Epsilon = 0.816\n",
      "Total steps: 514\n",
      "Episode 28: Total Reward = 21.00, Epsilon = 0.810\n",
      "Total steps: 533\n",
      "Episode 29: Total Reward = 19.00, Epsilon = 0.804\n",
      "Total steps: 555\n",
      "Episode 30: Total Reward = 22.00, Epsilon = 0.798\n",
      "Total steps: 565\n",
      "Episode 31: Total Reward = 10.00, Epsilon = 0.792\n",
      "Total steps: 583\n",
      "Episode 32: Total Reward = 18.00, Epsilon = 0.786\n",
      "Total steps: 602\n",
      "Episode 33: Total Reward = 19.00, Epsilon = 0.780\n",
      "Total steps: 625\n",
      "Episode 34: Total Reward = 23.00, Epsilon = 0.774\n",
      "Total steps: 667\n",
      "Episode 35: Total Reward = 42.00, Epsilon = 0.768\n",
      "Total steps: 688\n",
      "Episode 36: Total Reward = 21.00, Epsilon = 0.763\n",
      "Total steps: 742\n",
      "Episode 37: Total Reward = 54.00, Epsilon = 0.757\n",
      "Total steps: 767\n",
      "Episode 38: Total Reward = 25.00, Epsilon = 0.751\n",
      "Total steps: 789\n",
      "Episode 39: Total Reward = 22.00, Epsilon = 0.746\n",
      "Total steps: 808\n",
      "Episode 40: Total Reward = 19.00, Epsilon = 0.740\n",
      "Total steps: 820\n",
      "Episode 41: Total Reward = 12.00, Epsilon = 0.734\n",
      "Total steps: 844\n",
      "Episode 42: Total Reward = 24.00, Epsilon = 0.729\n",
      "Total steps: 853\n",
      "Episode 43: Total Reward = 9.00, Epsilon = 0.723\n",
      "Total steps: 864\n",
      "Episode 44: Total Reward = 11.00, Epsilon = 0.718\n",
      "Total steps: 885\n",
      "Episode 45: Total Reward = 21.00, Epsilon = 0.713\n",
      "Total steps: 934\n",
      "Episode 46: Total Reward = 49.00, Epsilon = 0.707\n",
      "Total steps: 948\n",
      "Episode 47: Total Reward = 14.00, Epsilon = 0.702\n",
      "Total steps: 985\n",
      "Episode 48: Total Reward = 37.00, Epsilon = 0.697\n",
      "Total steps: 1007\n",
      "Episode 49: Total Reward = 22.00, Epsilon = 0.692\n",
      "Total steps: 1073\n",
      "Episode 50: Total Reward = 66.00, Epsilon = 0.686\n",
      "Total steps: 1122\n",
      "Episode 51: Total Reward = 49.00, Epsilon = 0.681\n",
      "Total steps: 1142\n",
      "Episode 52: Total Reward = 20.00, Epsilon = 0.676\n",
      "Total steps: 1175\n",
      "Episode 53: Total Reward = 33.00, Epsilon = 0.671\n",
      "Total steps: 1217\n",
      "Episode 54: Total Reward = 42.00, Epsilon = 0.666\n",
      "Total steps: 1247\n",
      "Episode 55: Total Reward = 30.00, Epsilon = 0.661\n",
      "Total steps: 1257\n",
      "Episode 56: Total Reward = 10.00, Epsilon = 0.656\n",
      "Total steps: 1285\n",
      "Episode 57: Total Reward = 28.00, Epsilon = 0.651\n",
      "Total steps: 1320\n",
      "Episode 58: Total Reward = 35.00, Epsilon = 0.646\n",
      "Total steps: 1337\n",
      "Episode 59: Total Reward = 17.00, Epsilon = 0.641\n",
      "Total steps: 1357\n",
      "Episode 60: Total Reward = 20.00, Epsilon = 0.637\n",
      "Total steps: 1417\n",
      "Episode 61: Total Reward = 60.00, Epsilon = 0.632\n",
      "Total steps: 1541\n",
      "Episode 62: Total Reward = 124.00, Epsilon = 0.627\n",
      "Total steps: 1569\n",
      "Episode 63: Total Reward = 28.00, Epsilon = 0.622\n",
      "Total steps: 1587\n",
      "Episode 64: Total Reward = 18.00, Epsilon = 0.618\n",
      "Total steps: 1638\n",
      "Episode 65: Total Reward = 51.00, Epsilon = 0.613\n",
      "Total steps: 1672\n",
      "Episode 66: Total Reward = 34.00, Epsilon = 0.608\n",
      "Total steps: 1698\n",
      "Episode 67: Total Reward = 26.00, Epsilon = 0.604\n",
      "Total steps: 1717\n",
      "Episode 68: Total Reward = 19.00, Epsilon = 0.599\n",
      "Total steps: 1792\n",
      "Episode 69: Total Reward = 75.00, Epsilon = 0.595\n",
      "Total steps: 1818\n",
      "Episode 70: Total Reward = 26.00, Epsilon = 0.590\n",
      "Total steps: 1836\n",
      "Episode 71: Total Reward = 18.00, Epsilon = 0.586\n",
      "Total steps: 1877\n",
      "Episode 72: Total Reward = 41.00, Epsilon = 0.582\n",
      "Total steps: 1898\n",
      "Episode 73: Total Reward = 21.00, Epsilon = 0.577\n",
      "Total steps: 1925\n",
      "Episode 74: Total Reward = 27.00, Epsilon = 0.573\n",
      "Total steps: 1990\n",
      "Episode 75: Total Reward = 65.00, Epsilon = 0.569\n",
      "Total steps: 2037\n",
      "Episode 76: Total Reward = 47.00, Epsilon = 0.564\n",
      "Total steps: 2069\n",
      "Episode 77: Total Reward = 32.00, Epsilon = 0.560\n",
      "Total steps: 2179\n",
      "Episode 78: Total Reward = 110.00, Epsilon = 0.556\n",
      "Total steps: 2236\n",
      "Episode 79: Total Reward = 57.00, Epsilon = 0.552\n",
      "Total steps: 2299\n",
      "Episode 80: Total Reward = 63.00, Epsilon = 0.548\n",
      "Total steps: 2316\n",
      "Episode 81: Total Reward = 17.00, Epsilon = 0.543\n",
      "Total steps: 2359\n",
      "Episode 82: Total Reward = 43.00, Epsilon = 0.539\n",
      "Total steps: 2428\n",
      "Episode 83: Total Reward = 69.00, Epsilon = 0.535\n",
      "Total steps: 2554\n",
      "Episode 84: Total Reward = 126.00, Epsilon = 0.531\n",
      "Total steps: 2632\n",
      "Episode 85: Total Reward = 78.00, Epsilon = 0.527\n",
      "Total steps: 2822\n",
      "Episode 86: Total Reward = 190.00, Epsilon = 0.523\n",
      "Total steps: 2897\n",
      "Episode 87: Total Reward = 75.00, Epsilon = 0.519\n",
      "Total steps: 2930\n",
      "Episode 88: Total Reward = 33.00, Epsilon = 0.516\n",
      "Total steps: 2984\n",
      "Episode 89: Total Reward = 54.00, Epsilon = 0.512\n",
      "Total steps: 3173\n",
      "Episode 90: Total Reward = 189.00, Epsilon = 0.508\n",
      "Total steps: 3308\n",
      "Episode 91: Total Reward = 135.00, Epsilon = 0.504\n",
      "Total steps: 3577\n",
      "Episode 92: Total Reward = 269.00, Epsilon = 0.500\n",
      "Total steps: 3813\n",
      "Episode 93: Total Reward = 236.00, Epsilon = 0.497\n",
      "Total steps: 3856\n",
      "Episode 94: Total Reward = 43.00, Epsilon = 0.493\n",
      "Total steps: 3910\n",
      "Episode 95: Total Reward = 54.00, Epsilon = 0.489\n",
      "Total steps: 4116\n",
      "Episode 96: Total Reward = 206.00, Epsilon = 0.485\n",
      "Total steps: 4309\n",
      "Episode 97: Total Reward = 193.00, Epsilon = 0.482\n",
      "Total steps: 4365\n",
      "Episode 98: Total Reward = 56.00, Epsilon = 0.478\n",
      "Total steps: 4450\n",
      "Episode 99: Total Reward = 85.00, Epsilon = 0.475\n",
      "Total steps: 4480\n",
      "Episode 100: Total Reward = 30.00, Epsilon = 0.471\n",
      "Total steps: 4722\n",
      "Episode 101: Total Reward = 242.00, Epsilon = 0.468\n",
      "Total steps: 4947\n",
      "Episode 102: Total Reward = 225.00, Epsilon = 0.464\n",
      "Total steps: 5120\n",
      "Episode 103: Total Reward = 173.00, Epsilon = 0.461\n",
      "Total steps: 5140\n",
      "Episode 104: Total Reward = 20.00, Epsilon = 0.457\n",
      "Total steps: 5271\n",
      "Episode 105: Total Reward = 131.00, Epsilon = 0.454\n",
      "Total steps: 5317\n",
      "Episode 106: Total Reward = 46.00, Epsilon = 0.450\n",
      "Total steps: 5344\n",
      "Episode 107: Total Reward = 27.00, Epsilon = 0.447\n",
      "Total steps: 5357\n",
      "Episode 108: Total Reward = 13.00, Epsilon = 0.444\n",
      "Total steps: 5466\n",
      "Episode 109: Total Reward = 109.00, Epsilon = 0.440\n",
      "Total steps: 5504\n",
      "Episode 110: Total Reward = 38.00, Epsilon = 0.437\n",
      "Total steps: 5524\n",
      "Episode 111: Total Reward = 20.00, Epsilon = 0.434\n",
      "Total steps: 5545\n",
      "Episode 112: Total Reward = 21.00, Epsilon = 0.430\n",
      "Total steps: 5688\n",
      "Episode 113: Total Reward = 143.00, Epsilon = 0.427\n",
      "Total steps: 5799\n",
      "Episode 114: Total Reward = 111.00, Epsilon = 0.424\n",
      "Total steps: 5933\n",
      "Episode 115: Total Reward = 134.00, Epsilon = 0.421\n",
      "Total steps: 6045\n",
      "Episode 116: Total Reward = 112.00, Epsilon = 0.418\n",
      "Total steps: 6263\n",
      "Episode 117: Total Reward = 218.00, Epsilon = 0.414\n",
      "Total steps: 6394\n",
      "Episode 118: Total Reward = 131.00, Epsilon = 0.411\n",
      "Total steps: 6502\n",
      "Episode 119: Total Reward = 108.00, Epsilon = 0.408\n",
      "Total steps: 6645\n",
      "Episode 120: Total Reward = 143.00, Epsilon = 0.405\n",
      "Total steps: 6782\n",
      "Episode 121: Total Reward = 137.00, Epsilon = 0.402\n",
      "Total steps: 6925\n",
      "Episode 122: Total Reward = 143.00, Epsilon = 0.399\n",
      "Total steps: 6976\n",
      "Episode 123: Total Reward = 51.00, Epsilon = 0.396\n",
      "Total steps: 7057\n",
      "Episode 124: Total Reward = 81.00, Epsilon = 0.393\n",
      "Total steps: 7180\n",
      "Episode 125: Total Reward = 123.00, Epsilon = 0.390\n",
      "Total steps: 7237\n",
      "Episode 126: Total Reward = 57.00, Epsilon = 0.387\n",
      "Total steps: 7438\n",
      "Episode 127: Total Reward = 201.00, Epsilon = 0.384\n",
      "Total steps: 7865\n",
      "Episode 128: Total Reward = 427.00, Epsilon = 0.382\n",
      "Total steps: 7890\n",
      "Episode 129: Total Reward = 25.00, Epsilon = 0.379\n",
      "Total steps: 8030\n",
      "Episode 130: Total Reward = 140.00, Epsilon = 0.376\n",
      "Total steps: 8183\n",
      "Episode 131: Total Reward = 153.00, Epsilon = 0.373\n",
      "Total steps: 8464\n",
      "Episode 132: Total Reward = 281.00, Epsilon = 0.370\n",
      "Total steps: 8819\n",
      "Episode 133: Total Reward = 355.00, Epsilon = 0.367\n",
      "Total steps: 9154\n",
      "Episode 134: Total Reward = 335.00, Epsilon = 0.365\n",
      "Total steps: 9491\n",
      "Episode 135: Total Reward = 337.00, Epsilon = 0.362\n",
      "Total steps: 9699\n",
      "Episode 136: Total Reward = 208.00, Epsilon = 0.359\n",
      "Total steps: 9930\n",
      "Episode 137: Total Reward = 231.00, Epsilon = 0.357\n",
      "Total steps: 10213\n",
      "Episode 138: Total Reward = 283.00, Epsilon = 0.354\n",
      "Total steps: 10436\n",
      "Episode 139: Total Reward = 223.00, Epsilon = 0.351\n",
      "Total steps: 10471\n",
      "Episode 140: Total Reward = 35.00, Epsilon = 0.349\n",
      "Total steps: 10491\n",
      "Episode 141: Total Reward = 20.00, Epsilon = 0.346\n",
      "Total steps: 10665\n",
      "Episode 142: Total Reward = 174.00, Epsilon = 0.343\n",
      "Total steps: 10813\n",
      "Episode 143: Total Reward = 148.00, Epsilon = 0.341\n",
      "Total steps: 10966\n",
      "Episode 144: Total Reward = 153.00, Epsilon = 0.338\n",
      "Total steps: 11193\n",
      "Episode 145: Total Reward = 227.00, Epsilon = 0.336\n",
      "Total steps: 11410\n",
      "Episode 146: Total Reward = 217.00, Epsilon = 0.333\n",
      "Total steps: 11555\n",
      "Episode 147: Total Reward = 145.00, Epsilon = 0.331\n",
      "Total steps: 12055\n",
      "Episode 148: Total Reward = 500.00, Epsilon = 0.328\n",
      "Total steps: 12555\n",
      "Episode 149: Total Reward = 500.00, Epsilon = 0.326\n",
      "Total steps: 13055\n",
      "Episode 150: Total Reward = 500.00, Epsilon = 0.323\n",
      "Total steps: 13427\n",
      "Episode 151: Total Reward = 372.00, Epsilon = 0.321\n",
      "Total steps: 13642\n",
      "Episode 152: Total Reward = 215.00, Epsilon = 0.318\n",
      "Total steps: 13910\n",
      "Episode 153: Total Reward = 268.00, Epsilon = 0.316\n",
      "Total steps: 13997\n",
      "Episode 154: Total Reward = 87.00, Epsilon = 0.314\n",
      "Total steps: 14005\n",
      "Episode 155: Total Reward = 8.00, Epsilon = 0.311\n",
      "Total steps: 14171\n",
      "Episode 156: Total Reward = 166.00, Epsilon = 0.309\n",
      "Total steps: 14265\n",
      "Episode 157: Total Reward = 94.00, Epsilon = 0.307\n",
      "Total steps: 14471\n",
      "Episode 158: Total Reward = 206.00, Epsilon = 0.304\n",
      "Total steps: 14628\n",
      "Episode 159: Total Reward = 157.00, Epsilon = 0.302\n",
      "Total steps: 14647\n",
      "Episode 160: Total Reward = 19.00, Epsilon = 0.300\n",
      "Total steps: 14814\n",
      "Episode 161: Total Reward = 167.00, Epsilon = 0.298\n",
      "Total steps: 14997\n",
      "Episode 162: Total Reward = 183.00, Epsilon = 0.295\n",
      "Total steps: 15186\n",
      "Episode 163: Total Reward = 189.00, Epsilon = 0.293\n",
      "Total steps: 15521\n",
      "Episode 164: Total Reward = 335.00, Epsilon = 0.291\n",
      "Total steps: 15690\n",
      "Episode 165: Total Reward = 169.00, Epsilon = 0.289\n",
      "Total steps: 15941\n",
      "Episode 166: Total Reward = 251.00, Epsilon = 0.287\n",
      "Total steps: 16108\n",
      "Episode 167: Total Reward = 167.00, Epsilon = 0.284\n",
      "Total steps: 16291\n",
      "Episode 168: Total Reward = 183.00, Epsilon = 0.282\n",
      "Total steps: 16527\n",
      "Episode 169: Total Reward = 236.00, Epsilon = 0.280\n",
      "Total steps: 16836\n",
      "Episode 170: Total Reward = 309.00, Epsilon = 0.278\n",
      "Total steps: 17336\n",
      "Episode 171: Total Reward = 500.00, Epsilon = 0.276\n",
      "Total steps: 17556\n",
      "Episode 172: Total Reward = 220.00, Epsilon = 0.274\n",
      "Total steps: 17836\n",
      "Episode 173: Total Reward = 280.00, Epsilon = 0.272\n",
      "Total steps: 17992\n",
      "Episode 174: Total Reward = 156.00, Epsilon = 0.270\n",
      "Total steps: 18237\n",
      "Episode 175: Total Reward = 245.00, Epsilon = 0.268\n",
      "Total steps: 18452\n",
      "Episode 176: Total Reward = 215.00, Epsilon = 0.266\n",
      "Total steps: 18952\n",
      "Episode 177: Total Reward = 500.00, Epsilon = 0.264\n",
      "Total steps: 19195\n",
      "Episode 178: Total Reward = 243.00, Epsilon = 0.262\n",
      "Total steps: 19437\n",
      "Episode 179: Total Reward = 242.00, Epsilon = 0.260\n",
      "Total steps: 19736\n",
      "Episode 180: Total Reward = 299.00, Epsilon = 0.258\n",
      "Total steps: 20115\n",
      "Episode 181: Total Reward = 379.00, Epsilon = 0.256\n",
      "Total steps: 20615\n",
      "Episode 182: Total Reward = 500.00, Epsilon = 0.254\n",
      "Total steps: 20964\n",
      "Episode 183: Total Reward = 349.00, Epsilon = 0.252\n",
      "Total steps: 21464\n",
      "Episode 184: Total Reward = 500.00, Epsilon = 0.250\n",
      "Total steps: 21649\n",
      "Episode 185: Total Reward = 185.00, Epsilon = 0.248\n",
      "Total steps: 22149\n",
      "Episode 186: Total Reward = 500.00, Epsilon = 0.247\n",
      "Total steps: 22515\n",
      "Episode 187: Total Reward = 366.00, Epsilon = 0.245\n",
      "Total steps: 22662\n",
      "Episode 188: Total Reward = 147.00, Epsilon = 0.243\n",
      "Total steps: 22841\n",
      "Episode 189: Total Reward = 179.00, Epsilon = 0.241\n",
      "Total steps: 23024\n",
      "Episode 190: Total Reward = 183.00, Epsilon = 0.239\n",
      "Total steps: 23216\n",
      "Episode 191: Total Reward = 192.00, Epsilon = 0.237\n",
      "Total steps: 23441\n",
      "Episode 192: Total Reward = 225.00, Epsilon = 0.236\n",
      "Total steps: 23730\n",
      "Episode 193: Total Reward = 289.00, Epsilon = 0.234\n",
      "Total steps: 24034\n",
      "Episode 194: Total Reward = 304.00, Epsilon = 0.232\n",
      "Total steps: 24340\n",
      "Episode 195: Total Reward = 306.00, Epsilon = 0.230\n",
      "Total steps: 24675\n",
      "Episode 196: Total Reward = 335.00, Epsilon = 0.229\n",
      "Total steps: 24887\n",
      "Episode 197: Total Reward = 212.00, Epsilon = 0.227\n",
      "Total steps: 25204\n",
      "Episode 198: Total Reward = 317.00, Epsilon = 0.225\n",
      "Total steps: 25590\n",
      "Episode 199: Total Reward = 386.00, Epsilon = 0.224\n",
      "Total steps: 25611\n",
      "Episode 200: Total Reward = 21.00, Epsilon = 0.222\n",
      "Total steps: 26111\n",
      "Episode 201: Total Reward = 500.00, Epsilon = 0.220\n",
      "Total steps: 26611\n",
      "Episode 202: Total Reward = 500.00, Epsilon = 0.219\n",
      "Total steps: 27111\n",
      "Episode 203: Total Reward = 500.00, Epsilon = 0.217\n",
      "Total steps: 27219\n",
      "Episode 204: Total Reward = 108.00, Epsilon = 0.215\n",
      "Total steps: 27577\n",
      "Episode 205: Total Reward = 358.00, Epsilon = 0.214\n",
      "Total steps: 28077\n",
      "Episode 206: Total Reward = 500.00, Epsilon = 0.212\n",
      "Total steps: 28408\n",
      "Episode 207: Total Reward = 331.00, Epsilon = 0.210\n",
      "Total steps: 28565\n",
      "Episode 208: Total Reward = 157.00, Epsilon = 0.209\n",
      "Total steps: 28753\n",
      "Episode 209: Total Reward = 188.00, Epsilon = 0.207\n",
      "Total steps: 28927\n",
      "Episode 210: Total Reward = 174.00, Epsilon = 0.206\n",
      "Total steps: 29320\n",
      "Episode 211: Total Reward = 393.00, Epsilon = 0.204\n",
      "Total steps: 29351\n",
      "Episode 212: Total Reward = 31.00, Epsilon = 0.203\n",
      "Total steps: 29465\n",
      "Episode 213: Total Reward = 114.00, Epsilon = 0.201\n",
      "Total steps: 29517\n",
      "Episode 214: Total Reward = 52.00, Epsilon = 0.200\n",
      "Total steps: 29618\n",
      "Episode 215: Total Reward = 101.00, Epsilon = 0.198\n",
      "Total steps: 29885\n",
      "Episode 216: Total Reward = 267.00, Epsilon = 0.197\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# start training until max_steps\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# decide what action to take (exploration/exploitation)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# take the action in the environment\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[14], line 20\u001b[0m, in \u001b[0;36mselect_action\u001b[1;34m(state, epsilon)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# there's going to be lots of Keras/Torch -based extra message (output)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# use io.capture_output() to suppress them\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mcapture_output() \u001b[38;5;28;01mas\u001b[39;00m captured:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# this is basically the same as getting the best value from the current\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Q-table in classical Q-learning\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mq_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# return best action decided by current model => either go left or right\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(q_values)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:499\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    497\u001b[0m ):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[1;32m--> 499\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:720\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m--> 720\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[0;32m    722\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[0;32m    723\u001b[0m         dataset\n\u001b[0;32m    724\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:140\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# performance.\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprefetch(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mslice_batch_indices\u001b[39m(indices):\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        A Dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2341\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m   2336\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2339\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[1;32m-> 2341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_unbounded_threadpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_unbounded_threadpool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:43\u001b[0m, in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m     38\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[0;32m     39\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m      \u001b[49m\u001b[43mforce_synchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m synchronous:\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:157\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, force_synchronous, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_force_synchronous \u001b[38;5;241m=\u001b[39m force_synchronous\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1256\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1255\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_concrete_function_garbage_collected(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1257\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1258\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1226\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1224\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1225\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m   1230\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1060\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1057\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1059\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1060\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:231\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[0;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:161\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m    160\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[1;32m--> 161\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m ret \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:130\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset.<locals>.permutation\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpermutation\u001b[39m(_):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# It turns out to be more performant to make a new set of indices\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# rather than reusing the same range Tensor. (presumably because of\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;66;03m# buffer forwarding.)\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mand\u001b[39;00m shuffle \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    132\u001b[0m         indices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(indices)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2041\u001b[0m, in \u001b[0;36mrange\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   2039\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m, [start, limit, delta]) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[0;32m   2040\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(start, tensor_lib\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m-> 2041\u001b[0m     start \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2042\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(limit, tensor_lib\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m   2043\u001b[0m     limit \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(limit, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:736\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[0;32m    735\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[1;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconstant\u001b[39m(\n\u001b[0;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:291\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_broadcast\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:289\u001b[0m, in \u001b[0;36m_create_graph_constant\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    287\u001b[0m dtype_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtensor_value\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    288\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensor_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value}\n\u001b[1;32m--> 289\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_callbacks\u001b[38;5;241m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[0;32m    293\u001b[0m   \u001b[38;5;66;03m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[0;32m    294\u001b[0m   \u001b[38;5;66;03m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m   callback_outputs \u001b[38;5;241m=\u001b[39m op_callbacks\u001b[38;5;241m.\u001b[39minvoke_op_callbacks(\n\u001b[0;32m    296\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[38;5;241m=\u001b[39mname, graph\u001b[38;5;241m=\u001b[39mg)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:614\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    612\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    613\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2705\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2702\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2703\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2705\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_node_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2706\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2707\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2708\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2709\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2710\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2711\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2712\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2713\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2714\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2715\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   2716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1200\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1197\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1200\u001b[0m c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\User\\ReinforcementLearning_2025\\ReinforcementLearning_2025\\venv310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1057\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1053\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(name),\n\u001b[0;32m   1054\u001b[0m                                          serialized)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1057\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_FinishOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_desc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1059\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's have some helper lists to capture data for metrics\n",
    "losses = []\n",
    "rewards = []\n",
    "variances = []\n",
    "\n",
    "# initalize total steps\n",
    "total_steps = 0\n",
    "\n",
    "# training loop -> compare this to previous Q-learning examples as well!\n",
    "for episode in range(max_episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    # start training until max_steps\n",
    "    for step in range(max_steps):\n",
    "        # decide what action to take (exploration/exploitation)\n",
    "        action = select_action(state, epsilon)\n",
    "\n",
    "        # take the action in the environment\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # store the transition into the replay buffer\n",
    "        replay_buffer.store((state, action, reward, next_state, done))\n",
    "\n",
    "        # update the state for future\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "\n",
    "        # PERFORM THE ACTUAL TRAINING STEP FOR THE Q-NETWORK\n",
    "        # if current total_steps is divisible by 4 EXACTLY =>\n",
    "        if total_steps % q_network_train_step_interval == 0:\n",
    "            loss = train_step()\n",
    "            losses.append(loss)\n",
    "\n",
    "        # UPDATE THE TARGET NETWORK AS WELL IF THE INTERVAL MATCHES\n",
    "        # in this case => if it's divisible exactly with 128\n",
    "        if total_steps % target_step_update_interval == 0:\n",
    "            # re-synchronize the target network\n",
    "            target_network.set_weights(q_network.get_weights())\n",
    "            \n",
    "        # if the game has ended\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # the usual, apply decay to epsilon\n",
    "    # basically => multiply current epsilon by 0.99 (reduce by 1%)\n",
    "    # but don't go below the minimum value\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # show progress while training\n",
    "    print(f\"Total steps: {total_steps}\")\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward:.2f}, Epsilon = {epsilon:.3f}\")\n",
    "\n",
    "    # let's stop the training if we are satisfied with our results\n",
    "    # basically if we have almost maxed out the score \n",
    "    # and the epsilon is no longer updating => no point continuing the training\n",
    "    # kind of a version of \"EarlyStop\"\n",
    "    if total_reward >= (max_steps - 5) and epsilon == epsilon_min:\n",
    "        print(f\"Solved Cart Pole in {episode + 1} episodes!\")\n",
    "        break\n",
    "\n",
    "    # log the results for metrics\n",
    "    rewards.append(total_reward)\n",
    "    variances.append(np.var(rewards))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics (total rewards, running mean, variance over time, losses)\n",
    "\n",
    "#### In the end of the day, in reinforcement learning applications, the actual performance of the agent matters most.\n",
    "\n",
    "#### However, if the agent does not work well, metrics become useful then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # visualize variance over time\n",
    "print(\"Variance over time\")\n",
    "plt.plot(variances, color=\"green\")\n",
    "\n",
    "# if variance rises but running mean doesn't, we might have an instability issue\n",
    "# in this case, we usually need to adjust training parameters, hyperparameters (especially epsilon decay)\n",
    "# or even the reward mechanism in the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize losses (neural network metrics)\n",
    "print(\"Neural network losses over time\")\n",
    "plt.plot(losses)\n",
    "\n",
    "# losses are as usual in neural networks => it can imply overfitting\n",
    "# but it's also quite normal that the losses fluctuate even if the agent works well\n",
    "# if agent does not work well => adjust hyperparameters, especially the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the rewards progression\n",
    "print(\"Rewards by episode\")\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the rewards -list into NumPy -array format\n",
    "rewards_arr = np.array(rewards)\n",
    "\n",
    "# define a window size for a moving average\n",
    "\n",
    "# 50 is okay for 15000 episodes as the minimum window size\n",
    "# if the running mean line is too noisy, adjust the window size\n",
    "# rule of thumb: many episodes => bigger window_ize\n",
    "# 15000 => 30-50 is okay for window size\n",
    "window_size = 40\n",
    "\n",
    "# normalize the rewards (manual MinMax -normalization)\n",
    "normalized_rewards = (rewards_arr - min(rewards_arr)) / (max(rewards_arr) - min(rewards_arr))\n",
    "\n",
    "# calculate the running mean\n",
    "running_mean = np.convolve(normalized_rewards, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "\n",
    "print(\"Rewards w/ running mean:\")\n",
    "# visualize rewards + RUNNING MEAN\n",
    "plt.plot(normalized_rewards)\n",
    "plt.plot(running_mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally: test the DQN-agent in the Cart Pole -environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new environment for the test\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "state, info = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "# play the environment\n",
    "for step in range(1000):\n",
    "    action = select_action(state, epsilon)\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    # visualize frame\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# print result\n",
    "print(f\"Total Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
